plot(x = 1:10, y = train.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Training Error", main = "Training error keeps going down")
plot(x = 1:10, y = test.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Testing Error", main = "Bias / Variance Tradeoff")
plot(x = 1:10, y = train.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Error", col="grey")
lines(x = 1:10, y = test.error, type='b', col="red")
plot(x = 1:10, y = train.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Error", col="grey", ylim=c(1,30))
lines(x = 1:10, y = test.error, type='b', col="red")
plot(x = 1:10, y = train.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Error", col="grey", ylim=c(1,30))
lines(x = 1:10, y = test.error, type='b', col="red")
plot(x = 1:10, y = train.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Error", col="grey", ylim=c(12,20))
lines(x = 1:10, y = test.error, type='b', col="red")
plot(x = 1:10, y = train.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Error", col="grey", ylim=c(15,20))
lines(x = 1:10, y = test.error, type='b', col="red")
plot(x = 1:10, y = train.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Error", col="grey", ylim=c(15,19))
lines(x = 1:10, y = test.error, type='b', col="red")
plot(x = 1:10, y = train.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Error", col="grey", ylim=c(15,19), c(“Health”,”Defense”))
plot(x = 1:10, y = train.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Error", col="grey", ylim=c(15,19))
legend(2000,9.5,c('Health','Defense'),lty=c(1,1),lwd=c(2.5,2.5),col=c('blue','red'))
lines(x = 1:10, y = test.error, type='b', col='red')
plot(x = 1:10, y = train.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Error", col="grey", ylim=c(15,19))
legend(2000,9.5,c('Health','Defense'),lty=c(1,1),lwd=c(2.5,2.5),col=c('blue','red'))
lines(x = 1:10, y = test.error, type='b', col='red')
legend('topright', names(a)[-1] ,
lty=1, col=c('red', 'blue', 'green',' brown'), bty='n', cex=.75)
plot(x = 1:10, y = train.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Error", col="grey", ylim=c(15,19))
lines(x = 1:10, y = test.error, type='b', col='red')
legend('topright', c('training', 'testing') ,
lty=1, col=c('grey', 'red'), bty='n', cex=.75)
head(Auto)
dim(Auto)
train = sample(392, 196)
train.error=rep (0,10)
test.error=rep (0,10)
for (i in 1:10){
lm.fit = lm(mpg ~ poly(displacement + horsepower + weight + acceleration, i), data=Auto, subset=train)
train.error[i] = mean((mpg - predict (lm.fit, Auto))[train ]^2)
test.error[i] = mean((mpg - predict (lm.fit, Auto))[-train ]^2)
}
train.error
test.error
plot(x = 1:10, y = train.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Training Error", main = "Training error keeps going down")
plot(x = 1:10, y = test.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Testing Error", main = "Bias / Variance Tradeoff")
head(Auto)
dim(Auto)
train = sample(392, 196)
train.error=rep (0,10)
test.error=rep (0,10)
for (i in 1:10){
lm.fit = lm(mpg ~ poly(displacement + horsepower + weight + acceleration, i), data=Auto, subset=train)
train.error[i] = mean((mpg - predict (lm.fit, Auto))[train ]^2)
test.error[i] = mean((mpg - predict (lm.fit, Auto))[-train ]^2)
}
train.error
test.error
plot(x = 1:10, y = train.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Training Error", main = "Training error keeps going down")
plot(x = 1:10, y = test.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Testing Error", main = "Bias / Variance Tradeoff")
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(stats)
library(boot)
library (ISLR)
set.seed(97)
mu.hat = mean(Boston$medv)
library(boot)
mean.fn=function (data,index){
return (mean(data[index]))
}
mean.boot <- boot(Boston$medv, mean.fn, R=1000)
mean.boot
t.test(Boston$medv)
median.hat = median(Boston$medv)
median.fn=function (data,index){
return (median(data[index]))
}
median.boot <- boot(Boston$medv, median.fn, R=1000)
median.boot
summary(Boston$medv)
quantile(Boston$medv)
q1.fn=function (data,index){
q = quantile(data[index]);
return (q[2])
}
q1.boot <- boot(Boston$medv, q1.fn, R=1000)
q1.boot
q3.fn=function (data,index){
q = quantile(data[index]);
return (q[4])
}
q3.boot <- boot(Boston$medv, q3.fn, R=1000)
q3.boot
head(Auto)
dim(Auto)
train = sample(392, 196)
train.error=rep (0,10)
test.error=rep (0,10)
for (i in 1:10){
lm.fit = lm(mpg ~ poly(displacement + horsepower + weight + acceleration, i), data=Auto, subset=train)
train.error[i] = mean((mpg - predict (lm.fit, Auto))[train ]^2)
test.error[i] = mean((mpg - predict (lm.fit, Auto))[-train ]^2)
}
head(Auto)
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(stats)
library(boot)
library (ISLR)
set.seed(97)
mu.hat = mean(Boston$medv)
library(boot)
mean.fn=function (data,index){
return (mean(data[index]))
}
mean.boot <- boot(Boston$medv, mean.fn, R=1000)
mean.boot
t.test(Boston$medv)
median.hat = median(Boston$medv)
median.fn=function (data,index){
return (median(data[index]))
}
median.boot <- boot(Boston$medv, median.fn, R=1000)
median.boot
summary(Boston$medv)
quantile(Boston$medv)
q1.fn=function (data,index){
q = quantile(data[index]);
return (q[2])
}
q1.boot <- boot(Boston$medv, q1.fn, R=1000)
q1.boot
q3.fn=function (data,index){
q = quantile(data[index]);
return (q[4])
}
q3.boot <- boot(Boston$medv, q3.fn, R=1000)
q3.boot
head(Auto)
dim(Auto)
train = sample(392, 196)
train.error=rep (0,10)
test.error=rep (0,10)
for (i in 1:10){
lm.fit = lm(mpg ~ poly(displacement + horsepower + weight + acceleration, i), data=Auto, subset=train)
train.error[i] = mean((mpg - predict (lm.fit, Auto))[train ]^2)
test.error[i] = mean((mpg - predict (lm.fit, Auto))[-train ]^2)
}
head(Auto)
dim(Auto)
train = sample(392, 196)
attach(Auto)
train.error=rep (0,10)
test.error=rep (0,10)
for (i in 1:10){
lm.fit = lm(mpg ~ poly(displacement + horsepower + weight + acceleration, i), data=Auto, subset=train)
train.error[i] = mean((mpg - predict (lm.fit, Auto))[train ]^2)
test.error[i] = mean((mpg - predict (lm.fit, Auto))[-train ]^2)
}
train.error
test.error
plot(x = 1:10, y = train.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Training Error", main = "Training error keeps going down")
plot(x = 1:10, y = test.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Testing Error", main = "Bias / Variance Tradeoff")
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(stats)
library(boot)
library (ISLR)
set.seed(97)
mu.hat = mean(Boston$medv)
library(boot)
mean.fn=function (data,index){
return (mean(data[index]))
}
mean.boot <- boot(Boston$medv, mean.fn, R=1000)
mean.boot
t.test(Boston$medv)
median.hat = median(Boston$medv)
median.fn=function (data,index){
return (median(data[index]))
}
median.boot <- boot(Boston$medv, median.fn, R=1000)
median.boot
summary(Boston$medv)
quantile(Boston$medv)
q1.fn=function (data,index){
q = quantile(data[index]);
return (q[2])
}
q1.boot <- boot(Boston$medv, q1.fn, R=1000)
q1.boot
q3.fn=function (data,index){
q = quantile(data[index]);
return (q[4])
}
q3.boot <- boot(Boston$medv, q3.fn, R=1000)
q3.boot
head(Auto)
dim(Auto)
train = sample(392, 196)
attach(Auto)
train.error=rep (0,10)
test.error=rep (0,10)
for (i in 1:10){
lm.fit = lm(mpg ~ poly(displacement + horsepower + weight + acceleration, i), data=Auto, subset=train)
train.error[i] = mean((mpg - predict (lm.fit, Auto))[train ]^2)
test.error[i] = mean((mpg - predict (lm.fit, Auto))[-train ]^2)
}
train.error
test.error
plot(x = 1:10, y = train.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Training Error", main = "Training error keeps going down")
plot(x = 1:10, y = test.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Testing Error", main = "Bias / Variance Tradeoff")
plot(x = 1:10, y = train.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Error", col="grey", ylim=c(15,19))
lines(x = 1:10, y = test.error, type='b', col='red')
legend('topright', c('training', 'testing') ,
lty=1, col=c('grey', 'red'), bty='n', cex=.75)
seed(5)
set.seed(5)
head(Auto)
dim(Auto)
train = sample(392, 196)
attach(Auto)
train.error=rep (0,10)
test.error=rep (0,10)
for (i in 1:10){
lm.fit = lm(mpg ~ poly(displacement + horsepower + weight + acceleration, i), data=Auto, subset=train)
train.error[i] = mean((mpg - predict (lm.fit, Auto))[train ]^2)
test.error[i] = mean((mpg - predict (lm.fit, Auto))[-train ]^2)
}
train.error
test.error
plot(x = 1:10, y = train.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Training Error", main = "Training error keeps going down")
plot(x = 1:10, y = test.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Testing Error", main = "Bias / Variance Tradeoff")
set.seed(5)
head(Auto)
dim(Auto)
train = sample(392, 196)
attach(Auto)
train.error=rep (0,10)
test.error=rep (0,10)
for (i in 1:10){
lm.fit = lm(mpg ~ poly(displacement + horsepower + weight + acceleration, i), data=Auto, subset=train)
train.error[i] = mean((mpg - predict (lm.fit, Auto))[train ]^2)
test.error[i] = mean((mpg - predict (lm.fit, Auto))[-train ]^2)
}
train.error
test.error
plot(x = 1:10, y = train.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Training Error", main = "Training error keeps going down")
plot(x = 1:10, y = test.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Testing Error", main = "Bias / Variance Tradeoff")
set.seed(3)
head(Auto)
dim(Auto)
train = sample(392, 196)
attach(Auto)
train.error=rep (0,10)
test.error=rep (0,10)
for (i in 1:10){
lm.fit = lm(mpg ~ poly(displacement + horsepower + weight + acceleration, i), data=Auto, subset=train)
train.error[i] = mean((mpg - predict (lm.fit, Auto))[train ]^2)
test.error[i] = mean((mpg - predict (lm.fit, Auto))[-train ]^2)
}
train.error
test.error
plot(x = 1:10, y = train.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Training Error", main = "Training error keeps going down")
plot(x = 1:10, y = test.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Testing Error", main = "Bias / Variance Tradeoff")
plot(x = 1:10, y = train.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Error", col="grey", ylim=c(15,20))
lines(x = 1:10, y = test.error, type='b', col='red')
legend('topright', c('training', 'testing') ,
lty=1, col=c('grey', 'red'), bty='n', cex=.75)
plot(x = 1:10, y = train.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Error", col="grey", ylim=c(15,21))
lines(x = 1:10, y = test.error, type='b', col='red')
legend('topright', c('training', 'testing') ,
lty=1, col=c('grey', 'red'), bty='n', cex=.75)
plot(x = 1:10, y = train.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Error", col="grey", ylim=c(14,22))
lines(x = 1:10, y = test.error, type='b', col='red')
legend('topright', c('training', 'testing') ,
lty=1, col=c('grey', 'red'), bty='n', cex=.75)
mu.hat = mean(Boston$medv)
mu.hat
sd(Boston$medv)
sd(Boston$medv) / sqrt()
dim(Boston$medv)
dim(Boston)
summary(Boston)
sd(Boston$medv) / sqrt(506)
se_mu_hat = sd(Boston$medv) / sqrt(506)
se_mu_hat
mean.fn=function (data,index){
return (mean(data[index]))
}
mean.boot <- boot(Boston$medv, mean.fn, R=1000)
mean.boot
t.test(Boston$medv)
mean.boot$t0
mean.boot$t
mean.boot$t
mean.boot$t0
mean(mean.boot$t)
bias = mean(mean.boot$t) - mean.boot$t0
bias
mean.boot$t0
mean(mean.boot$t)
median.hat = median(Boston$medv)
median.hat
median.fn=function (data,index){
return (median(data[index]))
}
median.boot <- boot(Boston$medv, median.fn, R=1000)
median.boot
summary(Boston$medv)
quantile(Boston$medv)
q1.fn=function (data,index){
q = quantile(data[index]);
return (q[2])
}
q1.boot <- boot(Boston$medv, q1.fn, R=1000)
q1.boot
q3.fn=function (data,index){
q = quantile(data[index]);
return (q[4])
}
q3.boot <- boot(Boston$medv, q3.fn, R=1000)
q3.boot
set.seed(97)
library(MASS)
library(stats)
library(boot)
library (ISLR)
set.seed(97)
mu.hat = mean(Boston$medv)
mu.hat
se_mu_hat = sd(Boston$medv) / sqrt(506)
se_mu_hat
library(boot)
mean.fn=function (data,index){
return (mean(data[index]))
}
mean.boot <- boot(Boston$medv, mean.fn, R=1000)
mean.boot
mean.fn=function (data,index){
return (mean(data[index]))
}
mean.boot <- boot(Boston$medv, mean.fn, R=1000)
mean.boot
mean.fn=function (data,index){
return (mean(data[index]))
}
set.seed(97)
mean.boot <- boot(Boston$medv, mean.fn, R=1000)
mean.boot
mean.fn=function (data,index){
return (mean(data[index]))
}
set.seed(97)
mean.boot <- boot(Boston$medv, mean.fn, R=1000)
mean.boot
mean.fn=function (data,index){
return (mean(data[index]))
}
set.seed(97)
mean.boot <- boot(Boston$medv, mean.fn, R=1000)
mean.boot
mean.fn=function (data,index){
return (mean(data[index]))
}
set.seed(1)
mean.boot <- boot(Boston$medv, mean.fn, R=1000)
mean.boot
se_mu_hat = sd(Boston$medv) / sqrt(506)
se_mu_hat
mean.fn=function (data,index){
return (mean(data[index]))
}
set.seed(1)
mean.boot <- boot(Boston$medv, mean.fn, R=1000)
mean.boot
mean.fn=function (data,index){
return (mean(data[index]))
}
set.seed(2)
mean.boot <- boot(Boston$medv, mean.fn, R=1000)
mean.boot
mean.fn=function (data,index){
return (mean(data[index]))
}
set.seed(3)
mean.boot <- boot(Boston$medv, mean.fn, R=1000)
mean.boot
mean.fn=function (data,index){
return (mean(data[index]))
}
set.seed(3)
mean.boot <- boot(Boston$medv, mean.fn, R=1000)
mean.boot
t.test(Boston$medv)
mean.boot$t0
mean(mean.boot$t)
bias = mean(mean.boot$t) - mean.boot$t0
bias
mean.boot$t0
mean(mean.boot$t)
bias = mean(mean.boot$t) - mean.boot$t0
bias
mean.boot$t0
mean(mean.boot$t)
bias = mean(mean.boot$t) - mean.boot$t0
bias
mean.fn=function (data,index){
return (mean(data[index]))
}
set.seed(3)
mean.boot <- boot(Boston$medv, mean.fn, R=1000)
mean.boot
t.test(Boston$medv)
mean.boot$t0
mean(mean.boot$t)
bias = mean(mean.boot$t) - mean.boot$t0
bias
median.hat = median(Boston$medv)
median.hat
median.fn=function (data,index){
return (median(data[index]))
}
median.boot <- boot(Boston$medv, median.fn, R=1000)
median.boot
median.fn=function (data,index){
return (median(data[index]))
}
set.seed(3)
median.boot <- boot(Boston$medv, median.fn, R=1000)
median.boot
median.fn=function (data,index){
return (median(data[index]))
}
set.seed(3)
median.boot <- boot(Boston$medv, median.fn, R=1000)
median.boot
summary(Boston$medv)
quantile(Boston$medv)
q1.fn=function (data,index){
q = quantile(data[index]);
return (q[2])
}
set.seed(1)
q1.boot <- boot(Boston$medv, q1.fn, R=1000)
q1.boot
q3.fn=function (data,index){
q = quantile(data[index]);
return (q[4])
}
q3.boot <- boot(Boston$medv, q3.fn, R=1000)
q3.boot
q1.fn=function (data,index){
q = quantile(data[index]);
return (q[2])
}
set.seed(1)
q1.boot <- boot(Boston$medv, q1.fn, R=1000)
q1.boot
q3.fn=function (data,index){
q = quantile(data[index]);
return (q[4])
}
q3.boot <- boot(Boston$medv, q3.fn, R=1000)
q3.boot
q1.fn=function (data,index){
q = quantile(data[index]);
return (q[2])
}
set.seed(1)
q1.boot <- boot(Boston$medv, q1.fn, R=1000)
q1.boot
q3.fn=function (data,index){
q = quantile(data[index]);
return (q[4])
}
q3.boot <- boot(Boston$medv, q3.fn, R=1000)
q3.boot
set.seed(3)
head(Auto)
dim(Auto)
train = sample(392, 196)
attach(Auto)
train.error=rep (0,10)
test.error=rep (0,10)
for (i in 1:10){
lm.fit = lm(mpg ~ poly(displacement + horsepower + weight + acceleration, i), data=Auto, subset=train)
train.error[i] = mean((mpg - predict (lm.fit, Auto))[train ]^2)
test.error[i] = mean((mpg - predict (lm.fit, Auto))[-train ]^2)
}
train.error
test.error
plot(x = 1:10, y = train.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Training Error", main = "Training error keeps going down")
plot(x = 1:10, y = test.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Testing Error", main = "Bias / Variance Tradeoff")
plot(x = 1:10, y = train.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Error", col="grey", ylim=c(14,21))
lines(x = 1:10, y = test.error, type='b', col='red')
legend('topright', c('training', 'testing') ,
lty=1, col=c('grey', 'red'), bty='n', cex=.75)
head(Auto)
dim(Auto)
set.seed(3)
head(Auto)
dim(Auto)
train = sample(392, 196)
attach(Auto)
train.error=rep (0,10)
test.error=rep (0,10)
for (i in 1:10){
lm.fit = lm(mpg ~ poly(displacement + horsepower + weight + acceleration, i), data=Auto, subset=train)
train.error[i] = mean((mpg - predict (lm.fit, Auto))[train ]^2)
test.error[i] = mean((mpg - predict (lm.fit, Auto))[-train ]^2)
}
train.error
test.error
plot(x = 1:10, y = train.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Training Error", main = "Training error keeps going down")
plot(x = 1:10, y = test.error, type='b',xlab = "Flexibility - Polynomial Degree", ylab = "Testing Error", main = "Bias / Variance Tradeoff")

pred.train = predict(svm_p, newdata=OJ.train)
table(pred=pred.train, truth=OJ.train$Purchase)
cm=confusionMatrix(data = pred.train, reference = OJ.train$Purchase)
cm$byClass
mean(pred.train == OJ.train$Purchase)
pred.test = predict(svm_p,newdata=OJ.test)
table(pred=pred.test, truth=OJ.test$Purchase)
cm=confusionMatrix(data = pred.test, reference = OJ.test$Purchase)
cm$byClass
mean(pred.test == OJ.test$Purchase)
pred.test = predict(svm_p1,newdata=OJ.test)
svm_p = svm(Purchase~.,data=OJ.train,kernel="polynomial", degree=10,cost=0.01)
pred.test = predict(svm_p,newdata=OJ.test)
table(pred=pred.test, truth=OJ.test$Purchase)
cm=confusionMatrix(data = pred.test, reference = OJ.test$Purchase)
cm$byClass
mean(pred.test == OJ.test$Purchase)
cost_range = c(.01, .02, .03, .05, .2, .4, .7, 1,2,3,4,5,6,7,8,9,10)
tune.out = tune(svm, Purchase~., data=OJ.train, kernel="polynomial", degree=2, ranges=list(cost=cost_range))
summary(tune.out)
bestmod =tune.out$best.model
summary (bestmod )
pred.train = predict(bestmod, newdata=OJ.train)
table(pred=pred.train, truth=OJ.train$Purchase)
cm=confusionMatrix(data = pred.train, reference = OJ.train$Purchase)
cm$byClass
mean(pred.train == OJ.train$Purchase)
pred.test = predict(bestmod,newdata=OJ.test)
table(pred=pred.test, truth=OJ.test$Purchase)
cm=confusionMatrix(data = pred.test, reference = OJ.test$Purchase)
cm$byClass
mean(pred.test == OJ.test$Purchase)
library(ISLR)
library(e1071)
library(caret)
library(knitr)
data("OJ")
str(OJ)
set.seed(35)
train=sample(nrow(OJ),800)
OJ.train = OJ[train,]
OJ.test = OJ[-train,]
svm_l = svm(Purchase~.,data=OJ.train,kernel="linear",cost=0.01)
summary(svm_l)
pred.train = predict(svm_l, newdata=OJ.train)
table(pred=pred.train, truth=OJ.train$Purchase)
cm=confusionMatrix(data = pred.train, reference = OJ.train$Purchase)
cm$byClass
mean(pred.train == OJ.train$Purchase)
pred.test = predict(svm_l,newdata=OJ.test)
table(pred=pred.test, truth=OJ.test$Purchase)
cm=confusionMatrix(data = pred.test, reference = OJ.test$Purchase)
cm$byClass
mean(pred.test == OJ.test$Purchase)
cost_range = c(.01, .02, .03, .05, .2, .4, .7, 1,2,3,4,5,6,7,8,9,10)
tune.out = tune(svm, Purchase~., data=OJ.train, kernel="linear",  ranges=list(cost=cost_range))
summary(tune.out)
set.seed(35)
cost_range = c(.01, .02, .03, .05, .2, .4, .7, 1,2,3,4,5,6,7,8,9,10)
tune.out = tune(svm, Purchase~., data=OJ.train, kernel="linear",  ranges=list(cost=cost_range))
summary(tune.out)
set.seed(35)
cost_range = c(.01, .02, .03, .05, .2, .4, .7, 1,2,3,4,5,6,7,8,9,10)
tune.out = tune(svm, Purchase~., data=OJ.train, kernel="linear",  ranges=list(cost=cost_range))
summary(tune.out)
set.seed(101)
cost_range = c(.01, .02, .03, .05, .2, .4, .7, 1,2,3,4,5,6,7,8,9,10)
tune.out = tune(svm, Purchase~., data=OJ.train, kernel="linear",  ranges=list(cost=cost_range))
summary(tune.out)
set.seed(10)
cost_range = c(.01, .02, .03, .05, .2, .4, .7, 1,2,3,4,5,6,7,8,9,10)
tune.out = tune(svm, Purchase~., data=OJ.train, kernel="linear",  ranges=list(cost=cost_range))
summary(tune.out)
set.seed(35)
cost_range = c(.01, .02, .03, .05, .2, .4, .7, 1,2,3,4,5,6,7,8,9,10)
tune.out = tune(svm, Purchase~., data=OJ.train, kernel="linear",  ranges=list(cost=cost_range))
summary(tune.out)
bestmod =tune.out$best.model
summary (bestmod )
pred.train = predict(bestmod, newdata=OJ.train)
table(pred=pred.train, truth=OJ.train$Purchase)
cm=confusionMatrix(data = pred.train, reference = OJ.train$Purchase)
cm$byClass
mean(pred.train == OJ.train$Purchase)
pred.test = predict(bestmod,newdata=OJ.test)
table(pred=pred.test, truth=OJ.test$Purchase)
cm=confusionMatrix(data = pred.test, reference = OJ.test$Purchase)
cm$byClass
mean(pred.test == OJ.test$Purchase)
svm_r = svm(Purchase~.,data=OJ.train,kernel="radial",cost=0.01)
summary(svm_r)
pred.train = predict(svm_r, newdata=OJ.train)
table(pred=pred.train, truth=OJ.train$Purchase)
cm=confusionMatrix(data = pred.train, reference = OJ.train$Purchase)
cm$byClass
mean(pred.train == OJ.train$Purchase)
pred.test = predict(svm_r,newdata=OJ.test)
table(pred=pred.test, truth=OJ.test$Purchase)
cm=confusionMatrix(data = pred.test, reference = OJ.test$Purchase)
cm$byClass
mean(pred.test == OJ.test$Purchase)
cost_range = c(.01, .02, .03, .05, .2, .4, .7, 1,2,3,4,5,6,7,8,9,10)
set.seed(35)
tune.out = tune(svm, Purchase~., data=OJ.train, kernel="radial",  ranges=list(cost=cost_range))
summary(tune.out)
set.seed(35)
tune.out = tune(svm, Purchase~., data=OJ.train, kernel="radial",  ranges=list(cost=cost_range))
summary(tune.out)
bestmod =tune.out$best.model
summary (bestmod )
pred.train = predict(bestmod, newdata=OJ.train)
table(pred=pred.train, truth=OJ.train$Purchase)
cm=confusionMatrix(data = pred.train, reference = OJ.train$Purchase)
cm$byClass
mean(pred.train == OJ.train$Purchase)
pred.test = predict(bestmod,newdata=OJ.test)
table(pred=pred.test, truth=OJ.test$Purchase)
cm=confusionMatrix(data = pred.test, reference = OJ.test$Purchase)
cm$byClass
mean(pred.test == OJ.test$Purchase)
svm_p = svm(Purchase~.,data=OJ.train,kernel="polynomial", degree=2,cost=0.01)
summary(svm_p)
pred.train = predict(svm_p, newdata=OJ.train)
table(pred=pred.train, truth=OJ.train$Purchase)
cm=confusionMatrix(data = pred.train, reference = OJ.train$Purchase)
cm$byClass
cm$byClass
mean(pred.train == OJ.train$Purchase)
pred.test = predict(svm_p,newdata=OJ.test)
table(pred=pred.test, truth=OJ.test$Purchase)
cm=confusionMatrix(data = pred.test, reference = OJ.test$Purchase)
cm$byClass
mean(pred.test == OJ.test$Purchase)
set.seed(35)
tune.out = tune(svm, Purchase~., data=OJ.train, kernel="polynomial", degree=2, ranges=list(cost=cost_range))
summary(tune.out)
bestmod =tune.out$best.model
summary (bestmod )
pred.train = predict(bestmod, newdata=OJ.train)
table(pred=pred.train, truth=OJ.train$Purchase)
cm=confusionMatrix(data = pred.train, reference = OJ.train$Purchase)
cm$byClass
mean(pred.train == OJ.train$Purchase)
pred.test = predict(bestmod,newdata=OJ.test)
table(pred=pred.test, truth=OJ.test$Purchase)
cm=confusionMatrix(data = pred.test, reference = OJ.test$Purchase)
cm$byClass
mean(pred.test == OJ.test$Purchase)
library (tree)
install.packages('tree')
source('C:/study/897/hw/rlab_11.R', echo=TRUE)
source('C:/study/897/hw/rlab_11.R', echo=TRUE)
plot(tree.carseats )
text(tree.carseats ,pretty =0)
tree.carseats
source('C:/study/897/hw/rlab_11.R', echo=TRUE)
attach (Carseats )
High=ifelse (Sales <=8," No"," Yes ")
Carseats =data.frame(Carseats ,High)
tree.carseats =tree(High~.-Sales ,Carseats )
tree.carseats
summary (tree.carseats )
attach (Carseats )
library (tree)
library (ISLR)
attach (Carseats )
High=ifelse (Sales <=8," No"," Yes ")
Carseats =data.frame(Carseats ,High)
tree.carseats =tree(High~.-Sales ,Carseats )
summary (tree.carseats )
tree.carseats
plot(tree.carseats )
text(tree.carseats ,pretty =0)
source('C:/study/897/hw/rlab_11.R', echo=TRUE)
tree.pred=predict (tree.carseats ,Carseats.test ,type ="class")
table(tree.pred ,High.test)
set.seed (3)
cv.carseats =cv.tree(tree.carseats ,FUN=prune.misclass )
names(cv.carseats )
cv.carseats
par(mfrow =c(1,2))
plot(cv.carseats$size ,cv.carseats$dev ,type="b")
plot(cv.carseats$k ,cv.carseats$dev ,type="b")
prune.carseats =prune.misclass (tree.carseats ,best =9)
plot(prune.carseats )
text(prune.carseats ,pretty =0)
tree.pred=predict (prune.carseats , Carseats.test ,type="class")
table(tree.pred ,High.test)
(94+60) /200
prune.carseats =prune.misclass (tree.carseats ,best =15)
plot(prune.carseats )
text(prune.carseats ,pretty =0)
tree.pred=predict (prune.carseats , Carseats.test ,type=" class ")
table(tree.pred ,High.test)
tree.pred=predict (prune.carseats , Carseats.test ,type="class")
table(tree.pred ,High.test)
(86+62) /200
prune.carseats
prune.carseats =prune.misclass (tree.carseats ,best =9)
plot(prune.carseats )
text(prune.carseats ,pretty =0)
tree.pred=predict (prune.carseats , Carseats.test ,type="class")
table(tree.pred ,High.test)
(94+60) /200
prune.carseats =prune.misclass (tree.carseats ,best =15)
plot(prune.carseats )
text(prune.carseats ,pretty =0)
tree.pred=predict (prune.carseats , Carseats.test ,type="class")
table(tree.pred ,High.test)
(86+62) /200
prune.carseats =prune.misclass (tree.carseats ,best =13)
tree.pred=predict (prune.carseats , Carseats.test ,type="class")
table(tree.pred ,High.test)
(91+63) /200
dim(Carseats.test)
91+63+21+25
library (MASS)
set.seed (1)
train = sample (1: nrow(Boston ), nrow(Boston )/2)
tree.boston =tree(medv~.,Boston ,subset =train)
summary (tree.boston )
plot(tree.boston )
text(tree.boston ,pretty =0)
cv.boston =cv.tree(tree.boston )
plot(cv.boston$size ,cv.boston$dev ,type='b')
prune.boston =prune.tree(tree.boston ,best =5)
plot(prune.boston )
text(prune.boston ,pretty =0)
yhat=predict (tree.boston ,newdata =Boston [-train ,])
boston.test=Boston [-train ," medv"]
plot(yhat ,boston.test)
abline (0,1)
mean((yhat -boston.test)^2)
yhat
boston.test
boston.test=Boston [-train ,"medv"]
plot(yhat ,boston.test)
abline (0,1)
mean((yhat -boston.test)^2)
summary (tree.boston )
summary (tree.boston )
tree.boston
library (randomForest)
set.seed (1)
bag.boston =randomForest(medv~.,data=Boston ,subset =train ,
mtry=13, importance =TRUE)
bag.boston
yhat.bag = predict (bag.boston ,newdata =Boston [-train ,])
plot(yhat.bag , boston.test)
abline (0,1)
mean(( yhat.bag -boston.test)^2)
bag.boston =randomForest(medv~.,data=Boston ,subset =train ,
mtry=13, ntree =25)
yhat.bag = predict (bag.boston ,newdata =Boston [-train ,])
mean(( yhat.bag -boston.test)^2)
set.seed (1)
rf.boston =randomForest(medv~.,data=Boston ,subset =train ,
mtry=6, importance =TRUE)
yhat.rf = predict (rf.boston ,newdata =Boston [-train ,])
mean(( yhat.rf -boston.test)^2)
importance (rf.boston )
varImpPlot (rf.boston )
library (randomForest)
set.seed (1)
bag.boston =randomForest(medv~.,data=Boston ,subset =train ,
mtry=13, importance =TRUE)
bag.boston
bag.boston =randomForest(medv~.,data=Boston ,subset=train,mtry=13, importance =TRUE)
library (MASS)
library (randomForest)
set.seed (1)
Boston
bag.boston =randomForest(medv~.,data=Boston ,subset=train,mtry=13, importance =TRUE)
library (MASS)
set.seed (1)
train = sample (1: nrow(Boston ), nrow(Boston )/2)
library (randomForest)
set.seed (1)
bag.boston =randomForest(medv~.,data=Boston ,subset=train,mtry=13, importance =TRUE)
bag.boston
yhat.bag = predict (bag.boston ,newdata =Boston [-train ,])
plot(yhat.bag , boston.test)
boston.test=Boston [-train ,"medv"]
plot(yhat.bag , boston.test)
abline (0,1)
mean(( yhat.bag -boston.test)^2)
bag.boston =randomForest(medv~.,data=Boston ,subset =train ,
mtry=13, ntree =25)
yhat.bag = predict (bag.boston ,newdata =Boston [-train ,])
mean(( yhat.bag -boston.test)^2)
set.seed (1)
rf.boston =randomForest(medv~.,data=Boston ,subset =train ,
mtry=6, importance =TRUE)
yhat.rf = predict (rf.boston ,newdata =Boston [-train ,])
mean(( yhat.rf -boston.test)^2)
importance (rf.boston )
varImpPlot (rf.boston )
source('C:/study/897/hw/rlab_11.R', echo=TRUE)
library (randomForest)
set.seed (1)
bag.boston =randomForest(medv~.,data=Boston ,subset=train,mtry=13, importance =TRUE)
bag.boston
yhat.bag = predict (bag.boston ,newdata =Boston [-train ,])
plot(yhat.bag , boston.test)
plot(yhat ,boston.test)
plot(yhat.bag , boston.test)
set.seed(1)
rf.boston =randomForest(medv~.,data=Boston ,subset =train ,importance =TRUE)
rf.boston
yhat.rf = predict (rf.boston ,newdata =Boston [-train ,])
mean(( yhat.rf -boston.test)^2)
set.seed (1)
rf.boston =randomForest(medv~.,data=Boston ,subset =train ,
mtry=6, importance =TRUE)
yhat.rf = predict (rf.boston ,newdata =Boston [-train ,])
mean(( yhat.rf -boston.test)^2)
source('C:/study/897/hw/rlab_11.R', echo=TRUE)
library (gbm)
install.packages('gbm')
library (gbm)
set.seed (1)
boost.boston =gbm(medv~.,data=Boston [train ,], distribution=
"gaussian",n.trees =5000 , interaction.depth =4)
summary (boost.boston )
par(mfrow =c(1,2))
plot(boost.boston ,i="rm")
plot(boost.boston ,i=" lstat")
yhat.boost=predict (boost.boston ,newdata =Boston [-train ,],
n.trees =5000)
mean(( yhat.boost -boston.test)^2)
boost.boston =gbm(medv~.,data=Boston [train ,], distribution=
"gaussian",n.trees =5000 , interaction.depth =4, shrinkage =0.2,
verbose =F)
yhat.boost=predict (boost.boston ,newdata =Boston [-train ,],
n.trees =5000)
mean(( yhat.boost -boston.test)^2)
plot(boost.boston ,i="rm")
plot(boost.boston ,i="lstat")
plot(boost.boston ,i="dis")
set.seed (1)
boost.boston =gbm(medv~.,data=Boston [train ,], distribution=
"gaussian",n.trees =5000 , interaction.depth =4)
summary (boost.boston )
par(mfrow =c(1,2))
plot(boost.boston ,i="rm")
plot(boost.boston ,i="lstat")
plot(boost.boston ,i="dis")
yhat.boost=predict (boost.boston ,newdata =Boston [-train ,],
n.trees =5000)
mean(( yhat.boost -boston.test)^2)
boost.boston =gbm(medv~.,data=Boston [train ,], distribution=
"gaussian",n.trees =5000 , interaction.depth =4, shrinkage =0.2,
verbose =F)
yhat.boost=predict (boost.boston ,newdata =Boston [-train ,],
n.trees =5000)
mean(( yhat.boost -boston.test)^2)
plot(boost.boston ,i="dis")
plot(boost.boston ,i="rm")
plot(boost.boston ,i="lstat")
plot(boost.boston ,i="dis")
boost.boston =gbm(medv~.,data=Boston [train ,], distribution=
"gaussian",
n.trees=5000, interaction.depth=4, shrinkage=0.01,
verbose =F)
yhat.boost=predict (boost.boston ,newdata =Boston [-train ,],
n.trees =5000)
mean(( yhat.boost -boston.test)^2)
boost.boston =gbm(medv~.,data=Boston [train ,], distribution=
"gaussian",
n.trees=5000, interaction.depth=3, shrinkage=0.01,
verbose =F)
yhat.boost=predict (boost.boston ,newdata =Boston [-train ,],
n.trees =5000)
mean(( yhat.boost -boston.test)^2)
set.seed(1)
boost.boston =gbm(medv~.,data=Boston [train ,], distribution=
"gaussian",
n.trees=5000, interaction.depth=3, shrinkage=0.01,
verbose =F)
yhat.boost=predict (boost.boston ,newdata =Boston [-train ,],
n.trees =5000)
mean(( yhat.boost -boston.test)^2)
set.seed(1)
boost.boston =gbm(medv~.,data=Boston [train ,], distribution=
"gaussian",
n.trees=5000, interaction.depth=4, shrinkage=0.01,
verbose =F)
yhat.boost=predict (boost.boston ,newdata =Boston [-train ,],
n.trees =5000)
mean(( yhat.boost -boston.test)^2)
q11data=readRDS("q11data.Rda")
View(q11data)
tree.q11 =tree(y~., q11data)
tree.q11 =tree(Y~., q11data)
summary (tree.q11 )
tree.q11
plot(tree.q11 )
text(tree.q11 ,pretty =0)
View(q11data)
df_test<-data.frame(13, 14, -0.3, 0.9, 0.2)
names(df_test)<-c("X1", "X2", "X3", "X4", "X5")
df_test
tree.pred=predict (tree, df_test,type ="class")
tree = text(tree.q11 ,pretty =0)
df_test<-data.frame(13, 14, -0.3, 0.9, 0.2)
names(df_test)<-c("X1", "X2", "X3", "X4", "X5")
tree.pred=predict (tree, df_test,type ="class")
tree.pred=predict (tree.q11, df_test,type ="class")
df_test<-data.frame(0, 13, 14, -0.3, 0.9, 0.2)
names(df_test)<-c("Y", "X1", "X2", "X3", "X4", "X5")
tree.pred=predict (tree.q11, df_test,type ="class")
str(q11data)
q11data$Y <- as.factor(q11data$Y)
str(q11data)
tree.q11 =tree(Y~., q11data)
summary (tree.q11 )
plot(tree.q11 )
text(tree.q11 ,pretty =0)
df_test<-data.frame(0, 13, 14, -0.3, 0.9, 0.2)
names(df_test)<-c("Y", "X1", "X2", "X3", "X4", "X5")
tree.pred=predict (tree.q11, df_test,type ="class")
tree.pred
summary(tree.pred)
table(tree.pred)
tree.pred
df_test<-data.frame(0, 14, 14, 0.1, 0.1, 0.1)
names(df_test)<-c("Y", "X1", "X2", "X3", "X4", "X5")
tree.pred=predict (tree.q11, df_test,type ="class")
predict (tree.q11, df_test,type ="class")
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
data("OJ")
str(OJ)
set.seed(35)
train=sample(nrow(OJ),800)
OJ.train = OJ[train,]
OJ.test = OJ[-train,]
View(OJ)
tree.oj <- tree(Purchase ~ ., data = OJ.train)
tree <- tree(Purchase ~ ., data = OJ.train)
summary(tree.oj)
summary(tree)
# there is no parameter called Buy
tree <- tree(Purchase ~ ., data = OJ.train)
summary(tree)
tree
plot(tree)
text(tree, pretty = 0)
plot(tree)
text(tree, pretty = 0)
plot(tree)
text(tree, pretty = 0)
tree.pred <- predict(tree, OJ.test, type = "class")
table(tree.pred, OJ.test$Purchase)
143+73+18+36
(143+73)/(143+73+18+36)
cv.tree <- cv.tree(tree, FUN = prune.misclass)
cv.tree
plot(cv.tree$size, cv.tree$dev, type = "b", xlab = "Tree size", ylab = "Cross-validated classification error rate")
which.min(cv.tree$dev)
min(cv.tree$dev)
cv.tree <- cv.tree(tree, FUN = prune.misclass)
cv.tree
set.seed(35)
cv.tree <- cv.tree(tree, FUN = prune.misclass)
cv.tree
set.seed(1)
cv.tree <- cv.tree(tree, FUN = prune.misclass)
cv.tree
set.seed(1)
cv.tree <- cv.tree(tree, FUN = prune.misclass)
cv.tree
set.seed(1)
cv.tree <- cv.tree(tree, FUN = prune.misclass)
cv.tree
set.seed(1)
cv.tree <- cv.tree(tree, FUN = prune.misclass)
cv.tree
plot(cv.tree$size, cv.tree$dev, type = "b", xlab = "Tree size", ylab = "Cross-validated classification error rate")
prune.tree <- prune.misclass(tree, best = 2)
plot(prune.tree)
text(prune.tree, pretty = 0)
summary(tree)
summary(prune.tree)
prune.tree.pred <- predict(prune.tree, OJ.test, type = "class")
table(prune.tree.pred, OJ.test$Purchase)
(140+74)/(140+74+21+35)
1-(140+74)/(140+74+21+35)
plot(dtree)
plot(tree)
set.seed(35)
train=sample(nrow(OJ),800)
OJ.train = OJ[train,]
OJ.test = OJ[-train,]
# there is no parameter called Buy
dtree <- tree(Purchase ~ ., data = OJ.train)
summary(dtree)
plot(dtree)
text(dtree, pretty = 0)
1-(143+73)/(143+73+18+36)
set.seed(1)
cv.dtree <- cv.tree(dtree, FUN = prune.misclass)
cv.dtree
plot(cv.dtree$size, cv.dtree$dev, type = "b", xlab = "Tree size", ylab = "Cross-validated classification error rate")
prune.dtree <- prune.misclass(dtree, best = 2)
plot(prune.dtree)
text(prune.dtree, pretty = 0)
summary(dtree)
summary(prune.dtree)
prune.dtree.pred <- predict(prune.dtree, OJ.test, type = "class")
table(prune.dtree.pred, OJ.test$Purchase)
1-(140+74)/(140+74+21+35)

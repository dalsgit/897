cov = cov(spreturns_normalized)
R <- cor(spreturns_normalized)
corrplot.mixed(R)
heatmaply(R)
pr.out =prcomp (spreturns)
pr.var =pr.out$sdev ^2
pve=pr.var/sum(pr.var)
plot(pve , xlab="Principal Component",
ylab="Proportion of Variance Explained", ylim=c(0,1) ,type='b')
plot(cumsum (pve ), xlab="Principal Component", ylab ="Cumulative Proportion of Variance Explained",
ylim=c(0,1), type='b')
pve
round(pr.out$rotation[,1:2],2)
biplot(pr.out, cex=0.4)
round(pr.out$rotation[,1:2],2)
nComp = 2
Xhat = pr.out$x[,1:nComp] #%*% t(pr.out$rotation[,1:nComp])
cov(Xhat)
R <- cor(Xhat)
corrplot.mixed(R)
heatmaply(R)
cov(Xhat)
round(cov(Xhat))
plot(pr.out$x[, 1], pr.out$x[, 2], main = "PCA", xlab = "PC1", ylab = "PC2")
sv <- svd(spreturns_normalized)
plot(sv$u[, 1], sv$u[, 2], main = "SVD", xlab = "U1", ylab = "U2")
sv$v[1:5, 1:5]
pr.out$rotation[1:5, 1:5]
round(sv$v[1:5, 1:5] - pr.out$rotation[1:5, 1:5])
ylab="Proportion of Variance Explained", ylim=c(0,1) ,type='b')
plot(pve , xlab="Principal Component", ylab="Proportion of Variance Explained", ylim=c(0,1) ,type='b')
knitr::opts_chunk$set(echo = TRUE)
install.packages('heatmaply')
install.packages('heatmaply')
ibrary(heatmaply)
install.packages('knitr')
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
#C:\\study\\psu\\git\\897\\hw   C:\\study\\897\\hw
setwd("C:\\study\\psu\\git\\897\\hw")
#install.packages('corrplot')
#install.packages('heatmaply')
install.packages('knitr')
library(heatmaply)
library(corrplot)
install.packages("knitr")
knitr::opts_chunk$set(echo = TRUE)
install.packages('corrplot')
#C:\\study\\psu\\git\\897\\hw   C:\\study\\897\\hw
setwd("C:\\study\\psu\\git\\897\\hw")
#install.packages('corrplot')
#install.packages('heatmaply')
#install.packages('knitr')
library(heatmaply)
library(corrplot)
load("hw5_spreturns.Rda")
dim(spreturns)
colMeans(spreturns)
spreturns = spreturns[,1:100]
# Lets scale and center the spreturns object prior to PCA and SVD methods are used.
spreturns_normalized = scale(spreturns, center = TRUE, scale = TRUE)
remove(spreturns)
cov = cov(spreturns_normalized)
R <- cor(spreturns_normalized)
corrplot.mixed(R)
heatmaply(R)
always_allow_html: yes
knitr::opts_chunk$set(echo = TRUE, always_allow_html: yes
)
knitr::opts_chunk$set(echo = TRUE, always_allow_html=yes)
knitr::opts_chunk$set(echo = TRUE)
```{r setup, include=FALSE, always_allow_html: yes}
knitr::opts_chunk$set(echo = TRUE)
knit_with_parameters('C:/study/psu/git/897/hw/DAA06.Rmd')
library(leaps)
library(ISLR)
#install.packages('ISLR')
library(gam)
data("College")
attach(College)
set.seed (801)
trainingRows=sample (nrow(College), 600, replace = FALSE)
train = College[trainingRows,]
test = College[-trainingRows,]
regfit.fwd=regsubsets (Outstate~.,data=train, nvmax =17, method='forward')
reg.summary = summary (regfit.fwd)
plot(reg.summary$bic, xlab ="Number of Variables",ylab="BIC", type = 'l', main = 'Forward Step - Performance Measure')
which.min (reg.summary$bic )
points (which.min (reg.summary$bic ), reg.summary$bic[which.min (reg.summary$bic )], col ="red",cex =2, pch =20)
plot(reg.summary$cp, xlab ="Number of Variables",ylab="cp", type = 'l', main = 'Forward Step - Performance Measure')
which.min (reg.summary$cp )
points (which.min (reg.summary$cp ), reg.summary$cp[which.min (reg.summary$cp )], col ="red",cex =2, pch =20)
plot(reg.summary$adjr2, xlab ="Number of Variables",ylab="Adj R2", type = 'l', main = 'Forward Step - Performance Measure')
which.max (reg.summary$adjr2 )
points (which.max (reg.summary$adjr2 ), reg.summary$cp[which.max (reg.summary$adjr2 )], col ="red",cex =2, pch =20)
test.mat=model.matrix (Outstate~.,data=test)
test.val.errors =rep(NA ,17)
for(i in 1:17){
coefi=coef(regfit.fwd ,id=i)
pred=test.mat [,names(coefi)] %*% coefi
test.val.errors [i]= mean(( test$Apps-pred)^2)
}
plot(test.val.errors ,type='b', xlab='# of parameters', ylab='Test MSE')
names(coef(regfit.fwd ,id=6))
cut (Private,4)
fit.t= lm(Outstate~poly(Terminal ,5) ,data=train)
coef(summary(fit.t))
fit.pa= lm(Outstate~poly(perc.alumni ,5) ,data=train)
coef(summary(fit.pa))
View(test)
pred=predict (fit.6,newdata =test, se=T)
#"PrivateYes"  "Room.Board"  "Terminal"    "perc.alumni" "Expend"      "Grad.Rate"
fit.rb= lm(Outstate~poly(Room.Board ,5) ,data=train)
coef(summary(fit.rb))
# The relationship appears to be only linear
fit.t= lm(Outstate~poly(Terminal ,5) ,data=train)
coef(summary(fit.t))
# We have a cubic relationship, we can try ns
fit.pa= lm(Outstate~poly(perc.alumni ,5) ,data=train)
coef(summary(fit.pa))
# The relationship appears to be only linear
fit.e= lm(Outstate~poly(Expend ,5) ,data=train)
coef(summary(fit.e))
# We have a cubic relationship, we can try ns
fit.gr= lm(Outstate~poly(Grad.Rate ,5) ,data=train)
coef(summary(fit.gr))
# We have a cubic relationship, we can try ns
fit.1=gam(Outstate~Room.Board, data=train)
fit.2=gam(Outstate~Room.Board+poly(Terminal,3), data=train)
anova(fit.1, fit.2)
# 2nd is better, lets continue
fit.3=gam(Outstate~Room.Board+poly(Terminal,3)+perc.alumni, data=train)
anova(fit.1, fit.2, fit.3)
# 3rd is best, lets continue
fit.4=gam(Outstate~Room.Board+poly(Terminal,3)+perc.alumni+poly(Expend,3), data=train)
anova(fit.1, fit.2, fit.3, fit.4)
# 4th is best, lets continue
fit.5=gam(Outstate~Room.Board+poly(Terminal,3)+perc.alumni+poly(Expend,3)+poly(Grad.Rate,3), data=train)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
# 5th is best, lets continue
fit.6=gam(Outstate~Room.Board+poly(Terminal,3)+perc.alumni+poly(Expend,3)+poly(Grad.Rate,3)+Private, data=train)
anova(fit.1, fit.2, fit.3, fit.4, fit.5, fit.6)
# We will try a model with natural splines
fit.7=gam(Outstate~Room.Board+ns(Terminal)+perc.alumni+ns(Expend)+ns(Grad.Rate)+Private, data=train)
#Since model 6 and 7 are not nested, lets find the MSE on the test data to perform further comparison
pred=predict (fit.6,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
# Model with ns performs better
par(mfrow =c(3,3))
plot(fit.7, se=TRUE ,col ="blue ")
pred=predict (fit.7 ,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
pred=predict (fit.6,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
pred=predict (fit.7,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
fit.7=gam(Outstate~Room.Board+s(Terminal)+perc.alumni+s(Expend)+s(Grad.Rate)+Private, data=train)
fit.8=gam(Outstate~Room.Board+s(Terminal)+perc.alumni+s(Expend)+s(Grad.Rate)+Private, data=train)
pred=predict (fit.8,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
pred=predict (fit.8,newdata =test, se=T)
fit.8=gam(Outstate~Room.Board+s(Terminal)+perc.alumni+s(Expend)+s(Grad.Rate)+Private, data=train)
pred=predict (fit.6,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
pred=predict (fit.7,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
pred=predict (fit.8,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
pred=predict (fit.8,newdata =test)
mean((test$Outstate - pred$fit)^2)
pred=predict (fit.6,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
pred=predict (fit.7,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
library(leaps)
library(ISLR)
#install.packages('ISLR')
library(gam)
data("College")
attach(College)
set.seed (801)
trainingRows=sample (nrow(College), 600, replace = FALSE)
train = College[trainingRows,]
test = College[-trainingRows,]
regfit.fwd=regsubsets (Outstate~.,data=train, nvmax =17, method='forward')
reg.summary = summary (regfit.fwd)
plot(reg.summary$bic, xlab ="Number of Variables",ylab="BIC", type = 'l', main = 'Forward Step - Performance Measure')
which.min (reg.summary$bic )
points (which.min (reg.summary$bic ), reg.summary$bic[which.min (reg.summary$bic )], col ="red",cex =2, pch =20)
plot(reg.summary$cp, xlab ="Number of Variables",ylab="cp", type = 'l', main = 'Forward Step - Performance Measure')
which.min (reg.summary$cp )
points (which.min (reg.summary$cp ), reg.summary$cp[which.min (reg.summary$cp )], col ="red",cex =2, pch =20)
plot(reg.summary$adjr2, xlab ="Number of Variables",ylab="Adj R2", type = 'l', main = 'Forward Step - Performance Measure')
which.max (reg.summary$adjr2 )
points (which.max (reg.summary$adjr2 ), reg.summary$cp[which.max (reg.summary$adjr2 )], col ="red",cex =2, pch =20)
test.mat=model.matrix (Outstate~.,data=test)
test.val.errors =rep(NA ,17)
for(i in 1:17){
coefi=coef(regfit.fwd ,id=i)
pred=test.mat [,names(coefi)] %*% coefi
test.val.errors [i]= mean(( test$Apps-pred)^2)
}
plot(test.val.errors ,type='b', xlab='# of parameters', ylab='Test MSE')
#"PrivateYes"  "Room.Board"  "Terminal"    "perc.alumni" "Expend"      "Grad.Rate"
fit.rb= lm(Outstate~poly(Room.Board ,5) ,data=train)
coef(summary(fit.rb))
# The relationship appears to be only linear
fit.t= lm(Outstate~poly(Terminal ,5) ,data=train)
coef(summary(fit.t))
# We have a cubic relationship, we can try ns
fit.pa= lm(Outstate~poly(perc.alumni ,5) ,data=train)
coef(summary(fit.pa))
# The relationship appears to be only linear
fit.e= lm(Outstate~poly(Expend ,5) ,data=train)
coef(summary(fit.e))
# We have a cubic relationship, we can try ns
fit.gr= lm(Outstate~poly(Grad.Rate ,5) ,data=train)
coef(summary(fit.gr))
# We have a cubic relationship, we can try ns
fit.1=gam(Outstate~Room.Board, data=train)
fit.2=gam(Outstate~Room.Board+poly(Terminal,3), data=train)
anova(fit.1, fit.2)
# 2nd is better, lets continue
fit.3=gam(Outstate~Room.Board+poly(Terminal,3)+perc.alumni, data=train)
anova(fit.1, fit.2, fit.3)
# 3rd is best, lets continue
fit.4=gam(Outstate~Room.Board+poly(Terminal,3)+perc.alumni+poly(Expend,3), data=train)
anova(fit.1, fit.2, fit.3, fit.4)
# 4th is best, lets continue
fit.5=gam(Outstate~Room.Board+poly(Terminal,3)+perc.alumni+poly(Expend,3)+poly(Grad.Rate,3), data=train)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
# 5th is best, lets continue
fit.6=gam(Outstate~Room.Board+poly(Terminal,3)+perc.alumni+poly(Expend,3)+poly(Grad.Rate,3)+Private, data=train)
anova(fit.1, fit.2, fit.3, fit.4, fit.5, fit.6)
# We will try a model with natural splines
fit.7=gam(Outstate~Room.Board+ns(Terminal)+perc.alumni+ns(Expend)+ns(Grad.Rate)+Private, data=train)
#Since model 6 and 7 are not nested, lets find the MSE on the test data to perform further comparison
pred=predict (fit.6,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
pred=predict (fit.7,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
# Model with ns performs better
par(mfrow =c(3,3))
plot(fit.7, se=TRUE ,col ="blue ")
pred=predict (fit.7 ,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
pred=predict (fit.6,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
pred=predict (fit.7,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
fit.8=gam(Outstate~Room.Board+s(Terminal)+perc.alumni+s(Expend)+s(Grad.Rate)+Private, data=train)
pred=predict (fit.8,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
mean((test$Outstate - pred$fit)^2)
pred=predict (fit.7,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
par(mfrow =c(3,3))
plot(fit.6, se=TRUE ,col ="blue ")
plot(fit.6, se=TRUE ,col ="blue")
View(test)
fit.8=gam(Outstate~Room.Board+s(Terminal,3)+perc.alumni+s(Expend,3)+s(Grad.Rate,3)+Private, data=train)
pred=predict (fit.8,newdata =test, se=T)
fit.8=gam(Outstate~Room.Board+poly(Terminal,3)+perc.alumni+s(Expend,3)+s(Grad.Rate,3)+Private, data=train)
pred=predict (fit.8,newdata =test, se=T)
fit.8=gam(Outstate~Room.Board+poly(Terminal,3)+perc.alumni+poly(Expend,3)+s(Grad.Rate,3)+Private, data=train)
pred=predict (fit.8,newdata =test, se=T)
fit.8=gam(Outstate~Room.Board+poly(Terminal,3)+perc.alumni+poly(Expend,3)+poly(Grad.Rate,3)+Private, data=train)
pred=predict (fit.8,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
fit.8=gam(Outstate~Room.Board+poly(Terminal,3)+perc.alumni+s(Expend,3)+poly(Grad.Rate,3)+Private, data=train)
pred=predict (fit.8,newdata =test, se=T)
fit.8=gam(Outstate~Room.Board+s(Terminal,3)+perc.alumni+poly(Expend,3)+poly(Grad.Rate,3)+Private, data=train)
pred=predict (fit.8,newdata =test, se=T)
fit.8=gam(Outstate~Room.Board+poly(Terminal,3)+cut(perc.alumni,4)+poly(Expend,3)+poly(Grad.Rate,3)+Private, data=train)
pred=predict (fit.8,newdata =test, se=T)
fit.8=gam(Outstate~Room.Board+ns(Terminal,3)+perc.alumni+poly(Expend,3)+poly(Grad.Rate,3)+Private, data=train)
pred=predict (fit.8,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
pred=predict (fit.6,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
fit.8=gam(Outstate~Room.Board+ns(Terminal,3)+perc.alumni+ns(Expend,3)+poly(Grad.Rate,3)+Private, data=train)
pred=predict (fit.8,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
fit.8=gam(Outstate~Room.Board+ns(Terminal,3)+perc.alumni+ns(Expend,3)+ns(Grad.Rate,3)+Private, data=train)
pred=predict (fit.8,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
pred=predict (fit.7,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
fit.8=gam(Outstate~Room.Board+ns(Terminal,5)+perc.alumni+ns(Expend,5)+ns(Grad.Rate,5)+Private, data=train)
pred=predict (fit.8,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
fit.8=gam(Outstate~Room.Board+ns(Terminal,3)+perc.alumni+ns(Expend,3)+ns(Grad.Rate,3)+Private, data=train)
pred=predict (fit.8,newdata =test, se=T)
fit.7=gam(Outstate~Room.Board+ns(Terminal,3)+perc.alumni+ns(Expend,3)+ns(Grad.Rate,3)+Private, data=train)
fit.6=gam(Outstate~Room.Board+poly(Terminal,5)+perc.alumni+poly(Expend,5)+poly(Grad.Rate,5)+Private, data=train)
fit.8=gam(Outstate~Room.Board+ns(Terminal,3)+perc.alumni+ns(Expend,3)+ns(Grad.Rate,3)+Private, data=train)
pred=predict (fit.8,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
pred=predict (fit.6,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
#"PrivateYes"  "Room.Board"  "Terminal"    "perc.alumni" "Expend"      "Grad.Rate"
fit.rb= lm(Outstate~poly(Room.Board ,5) ,data=train)
coef(summary(fit.rb))
# The relationship appears to be only linear
fit.t= lm(Outstate~poly(Terminal ,5) ,data=train)
coef(summary(fit.t))
# We have a cubic relationship, we can try ns
fit.pa= lm(Outstate~poly(perc.alumni ,5) ,data=train)
coef(summary(fit.pa))
# The relationship appears to be only linear
fit.e= lm(Outstate~poly(Expend ,5) ,data=train)
coef(summary(fit.e))
# We have a cubic relationship, we can try ns
fit.gr= lm(Outstate~poly(Grad.Rate ,5) ,data=train)
coef(summary(fit.gr))
# We have a cubic relationship, we can try ns
fit.1=gam(Outstate~Room.Board, data=train)
fit.2=gam(Outstate~Room.Board+poly(Terminal,3), data=train)
anova(fit.1, fit.2)
# 2nd is better, lets continue
fit.3=gam(Outstate~Room.Board+poly(Terminal,3)+perc.alumni, data=train)
anova(fit.1, fit.2, fit.3)
# 3rd is best, lets continue
fit.4=gam(Outstate~Room.Board+poly(Terminal,3)+perc.alumni+poly(Expend,3), data=train)
anova(fit.1, fit.2, fit.3, fit.4)
# 4th is best, lets continue
fit.5=gam(Outstate~Room.Board+poly(Terminal,3)+perc.alumni+poly(Expend,3)+poly(Grad.Rate,3), data=train)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
# 5th is best, lets continue
fit.6=gam(Outstate~Room.Board+poly(Terminal,3)+perc.alumni+poly(Expend,3)+poly(Grad.Rate,3)+Private, data=train)
anova(fit.1, fit.2, fit.3, fit.4, fit.5, fit.6)
# We will try a model with natural splines
fit.7=gam(Outstate~Room.Board+ns(Terminal,3)+perc.alumni+ns(Expend,3)+ns(Grad.Rate,3)+Private, data=train)
#Since model 6 and 7 are not nested, lets find the MSE on the test data to perform further comparison
pred=predict (fit.6,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
pred=predict (fit.7,newdata =test, se=T)
mean((test$Outstate - pred$fit)^2)
# We get lower Test MSE with fit.7 (uses ns)
par(mfrow =c(3,3))
plot(fit.6, se=TRUE ,col ="blue")
pred=predict (regfit.fwd, newdata =test, se=T)
coefi=coef(regfit.fwd, id=0)
6
pred=test.mat [,names(coefi)] %*% coefi
mean(( test$Apps-pred)^2)
coefi=coef(regfit.fwd, id=7)
pred=test.mat [,names(coefi)] %*% coefi
mean(( test$Apps-pred)^2)
coefi=coef(regfit.fwd, id=13)
pred=test.mat [,names(coefi)] %*% coefi
mean(( test$Apps-pred)^2)
pred=predict (fit.7 ,newdata =test, se=T)
TestMSE_partB = mean((test$Outstate - pred$fit)^2)
TestMSE_partA / TestMSE_partB
TestMSE_partA = mean(( test$Apps-pred)^2)
pred=test.mat [,names(coefi)] %*% coefi
TestMSE_partA = mean(( test$Apps-pred)^2)
TestMSE_partA / TestMSE_partB
College[Private]
College[c(Private)]
dim(College[c(Private)])
dim(College[, c("Private")])
College[, c("Private")]
College[, c("Outstate", "Private", "Room.Board", "Terminal", "perc.alumni", "Expend", "Grad.Rate")]
pairs(College[, c("Outstate", "Private", "Room.Board", "Terminal", "perc.alumni", "Expend", "Grad.Rate")])
summary(fit.7)
source('C:/study/897/hw/rlab_9.r', echo=TRUE)
source('C:/study/897/hw/rlab_9.r', echo=TRUE)
glm.fits=glm(Direction~Lag1+Lag2 ,data=Smarket ,family =binomial ,subset =train)
summary (glm.fits)
source('C:/study/897/hw/rlab_9.r', echo=TRUE)
glm.probs =predict (glm.fits,Smarket.2005 , type="response")
glm.pred=rep ("Down " ,252)
glm.pred[glm.probs >.5]=" Up"
table(glm.pred ,Direction.2005)
contrasts(Direction.2005)
contrasts(glm.pred)
106/(106+35)
round(106/(106+35))
round(106/(106+35),1)
round(106/(106+35)*100,1)
round(106/(106+35)*100)
106/(106+35)*100
library (MASS)
lda.fit=lda(Direction~Lag1+Lag2 ,data=Smarket ,subset =train)
lda.fit
plot(lda.fit)
lda.pred=predict (lda.fit , Smarket.2005)
names(lda.pred)
lda.class =lda.pred$class
table(lda.class ,Direction.2005)
35/(35+76)*100
35/(35+76)
round(35/(35+76)*100)
lda.pred$posterior [1:20 ,1]
max(lda.pred$posterior [,1])
min(lda.pred$posterior [,1])
qda.fit=qda(Direction~Lag1+Lag2 ,data=Smarket ,subset =train)
qda.fit
qda.class =predict (qda.fit ,Smarket.2005) $class
table(qda.class ,Direction.2005)
table(Direction.2005, lda.class)
table(Direction.2005, qda.class)
121/(121+81)*100
library (class)
train.X=cbind(Lag1 ,Lag2)[train ,]
test.X=cbind (Lag1 ,Lag2)[!train ,]
train.Direction =Direction [train]
set.seed (1)
knn.pred=knn (train.X,test.X,train.Direction ,k=1)
table(knn.pred ,Direction.2005)
knn.pred=knn (train.X,test.X,train.Direction ,k=3)
table(knn.pred ,Direction.2005)
set.seed(1)
knn.pred=knn (train.X,test.X,train.Direction ,k=3)
table(knn.pred ,Direction.2005)
dim(Caravan )
attach (Caravan )
summary (Purchase )
standardized.X=scale(Caravan [,-86])
test =1:1000
train.X=standardized.X[-test ,]
test.X=standardized.X[test ,]
train.Y=Purchase [-test]
test.Y=Purchase [test]
set.seed (1)
knn.pred=knn (train.X,test.X,train.Y,k=1)
mean(test.Y!= knn.pred)
mean(test.Y!=" No")
mean(test.Y!="No")
table(knn.pred ,test.Y)
dim(Caravan )
attach (Caravan )
summary (Purchase )
standardized.X=scale(Caravan [,-86])
test =1:1000
train.X=standardized.X[-test ,]
test.X=standardized.X[test ,]
train.Y=Purchase [-test]
test.Y=Purchase [test]
set.seed (1)
knn.pred=knn (train.X,test.X,train.Y,k=1)
mean(test.Y!= knn.pred)
mean(test.Y!="No")
table(knn.pred ,test.Y)
knn.pred=knn (train.X,test.X,train.Y,k=3)
table(knn.pred ,test.Y)
knn.pred=knn (train.X,test.X,train.Y,k=5)
table(knn.pred ,test.Y)
glm.fits=glm(Purchase~.,data=Caravan ,family =binomial ,subset =-test)
glm.probs =predict ( glm.fits, Caravan [test,], type ="response")
glm.pred=rep ("No" ,1000)
glm.pred[glm.probs >.5]=" Yes "
table(glm.pred ,test.Y)
glm.pred=rep ("No" ,1000)
glm.pred[glm.probs >.25]=" Yes"
table(glm.pred ,test.Y)
lda.fit=lda(Purchase~.,data=Caravan,subset=-test)
lda.probs=predict(lda.fit, Caravan[test,])$posterior[,2]
lda.pred=rep("No",1000)
lda.pred[lda.probs>.25]="Yes"
table(lda.pred,test.Y)
contrasts(test.Y)
table(lda.pred,test.Y)
13/(46+13)
table(glm.pred ,test.Y)
11/(11+48)
dim(Caravan )
attach (Caravan )
summary (Purchase )
standardized.X=scale(Caravan [,-86])
test =1:1000
train.X=standardized.X[-test ,]
test.X=standardized.X[test ,]
train.Y=Purchase [-test]
test.Y=Purchase [test]
set.seed (1)
knn.pred=knn (train.X,test.X,train.Y,k=1)
mean(test.Y!= knn.pred)
mean(test.Y!="No")
table(knn.pred ,test.Y)
knn.pred=knn (train.X,test.X,train.Y,k=3)
table(knn.pred ,test.Y)
knn.pred=knn (train.X,test.X,train.Y,k=5)
table(knn.pred ,test.Y)
glm.fits=glm(Purchase~.,data=Caravan ,family =binomial ,subset =-test)
glm.probs =predict ( glm.fits, Caravan [test,], type ="response")
glm.pred=rep ("No" ,1000)
glm.pred[glm.probs >.25]="Yes"
table(glm.pred ,test.Y)
glm.fits=glm(Purchase~.,data=Caravan ,family =binomial ,subset =-test)
glm.probs =predict ( glm.fits, Caravan [test,], type ="response")
glm.pred=rep ("No" ,1000)
glm.pred[glm.probs >.25]="Yes"
table(glm.pred ,test.Y)
5
glm.pred[glm.probs >.5]="Yes"
glm.probs =predict ( glm.fits, Caravan [test,], type ="response")
glm.pred=rep ("No" ,1000)
glm.pred[glm.probs >.5]="Yes"
table(glm.pred ,test.Y)
glm.pred=rep ("No" ,1000)
glm.pred[glm.probs >.25]=" Yes"
table(glm.pred ,test.Y)
lda.fit=lda(Purchase~.,data=Caravan,subset=-test)
lda.probs=predict(lda.fit, Caravan[test,])$posterior[,2]
lda.pred=rep("No",1000)
lda.pred[lda.probs>.25]="Yes"
table(lda.pred,test.Y)
13/(46+13)
11/(11+48)

---
title: "Stat 897 Fall 2017 Data Analysis Assignment 5"
author: "Penn State"
date: "Due September 24, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### (1) In the first part of this assignment we will use the College data found in the ISLR library. Split the data into a training set of size 100 and test set with the rest. Your goal is to predict the number of applications received using the other variables in the data set.

```{r}
library(leaps)
library(ISLR)
library(quantmod)
library(glmnet)
#install.packages('plotmo')
library(plotmo)

data("College")

set.seed (1)
trainingRows=sample (nrow(College), 100, replace = FALSE)
train = College[trainingRows,]
test = College[-trainingRows,]
```

### (a) Fit a "best" model obtained from your previous assignment on the training set and report the test error for this model.

We will use the test MSE to find the best model

```{r}
regfit.best=regsubsets (Apps~.,data=train, nvmax=17)
test.mat=model.matrix (Apps~.,data=test)
test.val.errors =rep(NA ,17)
for(i in 1:17){
  coefi=coef(regfit.best ,id=i)
  pred=test.mat [,names(coefi)] %*% coefi
  test.val.errors [i]= mean(( test$Apps-pred)^2)
}
plot(test.val.errors ,type='b', xlab='# of parameters', ylab='Test MSE')
```

We see that we get the lowest test MSE for model with 11 parameters:
```{r}
which.min(test.val.errors)
test.val.errors[11]
```

The test MSE is reported to be 1624278

### (b) Fit a ridge regression model on the training set, with $\lambda$ chosen by cross-validation. Report the test error obtained.

```{r}
train.mat <- model.matrix(Apps ~ ., data = train)
test.mat <- model.matrix(Apps ~ ., data = test)
cv.ridge <- cv.glmnet(train.mat, train$Apps, alpha = 0)
bestlam.ridge <- cv.ridge$lambda.min
bestlam.ridge
```
We see that the value of lambda a that results in the smallest crossvalidation
error is 635. The test MSE associated with this value of lambda is:

```{r}
grid =10^ seq (10,-2, length =100)
fit.ridge =glmnet(train.mat, train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
pred.ridge = predict (fit.ridge, s=bestlam.ridge, newx=test.mat)
mean(( pred.ridge - test$Apps)^2)
```

The test MSE with ridge is: 1680229

### (c) Fit a lasso model on the training set, with $\lambda$ chosen by cross-validation. Report the test error obtained.

```{r}
cv.lasso <- cv.glmnet(train.mat, train$Apps, alpha = 1)
bestlam.lasso <- cv.lasso$lambda.min
bestlam.lasso
```
We see that the value of lambda a that results in the smallest crossvalidation
error is 73 The test MSE associated with this value of lambda is:

```{r}
fit.lasso <- glmnet(train.mat, train$Apps, alpha = 1, lambda = grid, thresh = 1e-12)
pred.lasso=predict (fit.lasso, s=bestlam.lasso, newx=test.mat)
mean(( pred.lasso - test$Apps)^2)
```

The test MSE with lasso is: 1621135

### (d) Compare the result obtained in (a) -- (c). How accurately can we predict the number of college applications? Is there much difference among the test errors resulting from different approaches?

We find that:
Best model with 11 parameters test MSE: 1624278
Ridge regression test MSE: 1680229
Lasso test MSE: 1621135

The test MSE is lowest for lasso, closely followed by best model with 11 parameters. Ridge regression has the highest MSE.

MSE for lasso and best model are relatively close than the ridge test MSE. 

### (e) Now partition the data into a training set of size 600 and a test set with the rest. Compare the test errors from the "best" linear regression model, ridge and lasso models. Note that, the "best" model here may not be the "best" model obtained before.

```{r}
trainingRows=sample (nrow(College), 600, replace = FALSE)
train = College[trainingRows,]
test = College[-trainingRows,]
```

We will use the test MSE to find the best model

```{r}
regfit.best=regsubsets (Apps~.,data=train, nvmax=17)
test.mat=model.matrix (Apps~.,data=test)
test.val.errors =rep(NA ,17)
for(i in 1:17){
  coefi=coef(regfit.best ,id=i)
  pred=test.mat [,names(coefi)] %*% coefi
  test.val.errors [i]= mean(( test$Apps-pred)^2)
}
plot(test.val.errors ,type='b', xlab='# of parameters', ylab='Test MSE')
```

```{r}
which.min(test.val.errors)
test.val.errors[which.min(test.val.errors)]
```
We see that we get the lowest test MSE for model with 13 parameters. It is a different result than the last time.

The test MSE for the selected model is: 561171.9

### (b) Fit a ridge regression model on the training set, with $\lambda$ chosen by cross-validation. Report the test error obtained.

```{r}
train.mat <- model.matrix(Apps ~ ., data = train)
test.mat <- model.matrix(Apps ~ ., data = test)
cv.ridge <- cv.glmnet(train.mat, train$Apps, alpha = 0)
bestlam.ridge <- cv.ridge$lambda.min
bestlam.ridge
```
We see that the value of lambda a that results in the smallest crossvalidation
error is 413. The test MSE associated with this value of lambda is:

```{r}
fit.ridge =glmnet(train.mat, train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
pred.ridge = predict (fit.ridge, s=bestlam.ridge, newx=test.mat)
mean(( pred.ridge - test$Apps)^2)
```
The test MSE: 691818.8

### (c) Fit a lasso model on the training set, with $\lambda$ chosen by cross-validation. Report the test error obtained.

```{r}
cv.lasso <- cv.glmnet(train.mat, train$Apps, alpha = 1)
bestlam.lasso <- cv.lasso$lambda.min
bestlam.lasso
```
We see that the value of lambda a that results in the smallest crossvalidation
error is 3. The test MSE associated with this value of lambda is:

```{r}
fit.lasso <- glmnet(train.mat, train$Apps, alpha = 1, lambda = grid, thresh = 1e-12)
pred.lasso=predict (fit.lasso, s=bestlam.lasso, newx=test.mat)
mean(( pred.lasso - test$Apps)^2)
```

The test MSE: 561649.3

### (f) Do you see any difference between the two sets of results? Comment.

We find that (600 training set):
Best model with 13 parameters test MSE: 561171.9
Ridge regression test MSE: 691818.8
Lasso test MSE: 561649.3

The overall MSE has reduced but we observe a similar pattern as before with the diff that the test MSE is lowest for best model, closely followed by lasso. Ridge regression has the highest MSE.

MSE for lasso and best model are relatively close than the ridge test MSE. 

(100 training set):
Best model with 11 parameters test MSE: 1624278
Ridge regression test MSE: 1680229
Lasso test MSE: 1621135


### (2) The file Sp.Rdv contains daily returns for 501 stocks in 2016.  It was created as follows.

### (You don't need to run this, i.e. leave eval=FALSE)
```{r, echo=TRUE, eval=FALSE}
library("quantmod")
sp = read.csv("SP500HistoricalComponents.csv")
Symbols = sp[which(sp$X12.31.2016 == "X"), "Ticker"]
StartDate = '2016-01-01'
EndDate = '2016-12-31'

Stocks = lapply(Symbols, function(sym) {
  print(sym)
  dailyReturn(na.omit(getSymbols(as.character(sym), from = StartDate, to = EndDate, 
                                 auto.assign = FALSE, src = "yahoo")))
})

SPreturns = do.call(merge, Stocks)
colnames(SPreturns) = Symbols

#Clean out cols with NA
spreturns = SPreturns[ , colSums(is.na(SPreturns)) == 0]
save(spreturns, file = "spreturns.Rda")
```

### Make sure you downloaded the data from the assignment page, and you can load it using:
```{r}
load("hw5_spreturns.Rda")
```

### I have constructed a secret long-only portfolio chosen from these stocks.  It contains between five and twenty stocks.  The daily return of this portfolio for each trading day of 2016 is in the object `portfolioreturns` which you can load from file with:
```{r}
load("hw5_portfolioreturnsstatic.Rda")
dim(spreturns)
dim(portfolioreturns)
```

### Your goal is to recover the stocks and weights of the secret portfolio.

### Note that you can think of a portfolio as a vector of nonnegative weights that sum to one.   For simplicity, we are assuming that this portfolio is rebalanced daily at the closing prices.  Then if the daily returns vector on date $d$ is $r_d$ and the weight vector is $w_d$, the daily return for the portfolio is the dot product $r_d \cdot w_d$.  If this were a buy-and-hold portfolio, we would have to back into the returns more carefully.

### (a)  First try to fit an ordinary regression (lm) with `portfolioreturns` as the response and `spreturns` as the predictors. What happens? What problem do you run into?

```{r}
options(max.print=1000000)
portWithReturnsAndStocks <- cbind(portfolioreturns, spreturns)
dim(portWithReturnsAndStocks)
colnames(portWithReturnsAndStocks)[1] <- "portfolioreturns"

lm.fit=lm(portfolioreturns~., data=portWithReturnsAndStocks)
summary(lm.fit)
options(max.print=252)

```

There is a clear problem of not having any significant parameter in the model. More variables than observations, so linear regression will not give a unique solution!

### (b) Now use elasticnet (Lasso, Ridge, or a combination, i.e. glmnet) instead of linear regression to model the secret portfolio. Once get a smaller set of variables from the shrinkage, refit a linear model with only those predictors. [Note, you may want to check out the `plotmo` and `pander` packages for nice plots and outputs for your models.]

```{r}
x=model.matrix (portfolioreturns~., portWithReturnsAndStocks)[,-1]; 
y=portWithReturnsAndStocks$portfolioreturns
grid =10^ seq (10,-2, length =100)
port.ridge =glmnet (x, y,alpha =0, thresh =1e-12)
plot_glmnet(port.ridge, label=20)  

### Use lasso with CV
cv.lasso <- cv.glmnet(x, y, alpha = 1)
bestlam.lasso <- cv.lasso$lambda.min
bestlam.lasso

port.lasso =glmnet (x, y, alpha =1, thresh =1e-12)
lasso.coef=predict (port.lasso, type ="coefficients", s=bestlam.lasso )[0:502,]

lasso.coef[lasso.coef !=0]

plot_glmnet(port.lasso, label=20) 

```
The list of parameters that we find significant from lasso are: ADP + AVGO +   CL + CTXS +   GE +  HPE +  HST  + MCO +  MHK +  NWL + PCLN +  PVH + TSCO + V 

Lets use these to perform linear regressions.

```{r}
lm.fit=lm(portfolioreturns ~ ADP + AVGO +   CL + CTXS +   GE +  HPE +  HST  + MCO +  MHK +  NWL + PCLN +  PVH + TSCO + V, data=portWithReturnsAndStocks)
summary(lm.fit)
 
```

We get much better results in this case.

### (c) Here is how the portfolio was created. We're essentially randomly sampling columns from spreturns to have non-zero coefficients (portfoliowts) and then generating returns from that.

```{r, echo=TRUE, eval=FALSE}
t = runif(ncol(spreturns))
thresh = .98
mask = t > thresh
w = runif(ncol(spreturns))
sum(mask) # number of chosen coefficients
portfoliowts = w * mask / sum(w * mask)
myportfolioreturns = spreturns %*% portfoliowts
```

### Write a function that takes a threshold as input and produces the total error for estimated weights from lasso to the true weights. To do this you will need (1) a function that generates weights and returns for a given threshold function, (2) a function that takes the the returns and outputs the estimated coefficeints from a lasso, and (3) a function that takes the estimated coefficients and returns the error relative to the true weights. Plot the errors for a variety of different thresholds between 0.5 and 1.

```{r}
generateWeightsAndReturns <- function(threshold) {
  t = runif(ncol(spreturns))
  mask = t > threshold
  w = runif(ncol(spreturns))
  portfoliowts = w * mask / sum(w * mask)
  list(Returns=spreturns %*% portfoliowts, Weights=portfoliowts) 
}

estimateLassoCoef <- function(returns) {
  portWithReturnsAndStocks <- cbind(returns, spreturns)
  dim(portWithReturnsAndStocks)
  colnames(portWithReturnsAndStocks)[1] <- "portfolioreturns"
  x=model.matrix (portfolioreturns ~ ., data = portWithReturnsAndStocks)[,-1]
  y=portWithReturnsAndStocks$portfolioreturns
  cv.lasso <- cv.glmnet(x,y, alpha = 1)
  bestlam.lasso <- cv.lasso$lambda.min

  lasso.mod =glmnet (x,y,alpha =1)
  lasso.coef  <- predict(lasso.mod, type = 'coefficients', s = bestlam.lasso)
  lasso.coef
}

# find squared differences 
squaredDifferenceBetweenEstimatedAndTrueWeights <- function(estCoef, trueCoef) {
  # The est coef include the intercept that we have to remove
  sum((estCoef[2:502] - trueCoef)^2)
}

wrapper <- function(threshold) {
  weightsAndReturns = generateWeightsAndReturns(threshold)
  squaredDifferenceBetweenEstimatedAndTrueWeights(estimateLassoCoef(weightsAndReturns$Returns), weightsAndReturns$Weights)
}

emitID <- local({
    idCounter <- 0
    function(){
        idCounter <<- idCounter + 1L                     # increment
        formatC(idCounter, width=9, flag=0, format="d")  # format & return
    }
})

df <- data.frame(thresholds=numeric(0), differences=numeric(0))
lapply(runif(20, min = 0.5, max = 1), function(x) { df[emitID(),] <<- c(x, wrapper(x))  } )

plot(df)


```
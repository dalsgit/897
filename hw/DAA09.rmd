---
title: "Stat 897 Fall 2017 Data Analysis Assignment 9"
author: "Penn State"
date: "Due October 29, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this assignment we will use the Boston data found in the MASS library.

### 1. Fit classification models in order to predict whether a given suburb has a crime rate above or below the median. Explore logistic regression, LDA, and KNN models using various subsets of the predictors. At this point in the class, you should feel fairly comfortable making such an open-ended exploration.

### Describe your findings, show appropriate results, and determine why some techniques perform better or worse and which had the best performance.

```{r}
library(MASS)
library(class)
library(glmnet)
library(leaps)
library(caret)

data("Boston")
attach(Boston)
```

Let's start with parameter selection for the Boston data set. We will use forward selection, lasso and ridge here:
```{r}
x = model.matrix(crim ~ . - 1, data = Boston)
y = Boston$crim

n = nrow(Boston)
p = ncol(Boston) - 1
set.seed (801)
trainingRows=sample (nrow(Boston), n*0.7, replace = FALSE)
train = Boston[trainingRows,]
test = Boston[-trainingRows,]
train.mat <- model.matrix(crim~ ., data = train)
test.mat <- model.matrix(crim~ ., data = test)

# Forward Selection | BIC
regfit.fwd=regsubsets (crim~.,data=train, nvmax =14, method='forward')
reg.summary = summary (regfit.fwd)
reg.summary
plot(reg.summary$bic, xlab ="Number of Variables",ylab="BIC", type = 'l', main = 'Forward Step - Performance Measure')
which.min (reg.summary$bic )
points (which.min (reg.summary$bic ), reg.summary$bic[which.min (reg.summary$bic )], col ="red",cex =2, pch =20)


#LASSO
grid =10^ seq (10,-2, length =100)
cv.lasso = cv.glmnet(x, y, type.measure = "mse", nfolds=10)
plot(cv.lasso)
bestlam.lasso=cv.lasso$lambda.min #find the best tuning parameter

fit.lasso <- glmnet(train.mat, train$crim, alpha = 1, lambda = grid, thresh = 1e-12)
pred.lasso=predict (fit.lasso, s=bestlam.lasso, newx=test.mat)
mean(( pred.lasso - test$crim)^2)

final.lasso=glmnet(x,y,alpha=1) #fit on the entire data set to extract coef
lasso.coef=predict(final.lasso,type="coefficients",s=bestlam.lasso)[1:14,]
lasso.coef
length(lasso.coef[lasso.coef !=0])
lasso.coef[lasso.coef!=0] #contains 11 variables in our model

#Ridge regression
cv.ridge = cv.glmnet(x, y, alpha=0, type.measure = "mse", nfolds=length(y),grouped=FALSE)
plot(cv.ridge)
bestlam.ridge=cv.ridge$lambda.min #find the best tuning parameter

fit.ridge =glmnet(train.mat, train$crim, alpha = 0, lambda = grid, thresh = 1e-12)
pred.ridge = predict (fit.ridge, s=bestlam.ridge, newx=test.mat)
mean(( pred.ridge - test$crim)^2)

final.ridge=glmnet(x,y,alpha=0) #fit on the full data
ridge.coef=predict(final.ridge,type="coefficients",s=bestlam.ridge)[1:14,]
ridge.coef
ridge.coef[ridge.coef!=0] #contains all variables in our model
```

Based on the results above we have the following:
Lasso: selects model with 11 variables:  zn + indus + chas + nox + rm + dis + rad + ptratio + black + lstat + medv

Forward selection:  selects model with 3 variables: rad + black + lstat

Ridge: selects all parameters - we will ignore this as we get smaller models with better Test MSE with Lasso.

```{r}
# do some data processing to prepare the categorical variable
crim_modified <- rep(0, length(crim))
crim_modified[crim > median(crim)] <- 1

# add new column to the data frame
Boston <- data.frame(Boston, crim_modified)
```

```{r}
train <- 1:(length(crim) * 0.7)
test <- (length(train)+ 1):length(crim)
Boston.train <- Boston[train, ]
Boston.test <- Boston[test, ]
crim_modified.test <- crim_modified[test]

# Logistic Regression: Model with all parameters
fit.glm <- glm(crim_modified ~ . - crim_modified - crim, data = Boston, family = binomial, subset = train)
probs <- predict(fit.glm, Boston.test, type = "response")
pred.glm <- rep(0, length(probs))
pred.glm[probs > 0.5] <- 1
conf_matrix = table(pred.glm, crim_modified.test)
conf_matrix
cm=confusionMatrix(data = pred.glm, reference = crim_modified.test)
cm$byClass
mean(pred.glm == crim_modified.test)
```

We see that the results appear good. However specificity is 41%

```{r}
# Logistic Regression: Model with parameters selected by lasso
fit.glm <- glm(crim_modified ~ zn + indus + chas + nox + rm + dis + rad + ptratio + black + lstat + medv, data = Boston, family = binomial, subset = train)
probs <- predict(fit.glm, Boston.test, type = "response")
pred.glm <- rep(0, length(probs))
pred.glm[probs > 0.5] <- 1
table(pred.glm, crim_modified.test)
cm=confusionMatrix(data = pred.glm, reference = crim_modified.test)
cm$byClass
mean(pred.glm == crim_modified.test)

```

Results similar to when we use all params.

```{r}
# Logistic Regression: Model with parameters selected by forward selection
fit.glm <- glm(crim_modified ~ rad + black + lstat, data = Boston, family = binomial, subset = train)
probs <- predict(fit.glm, Boston.test, type = "response")
pred.glm <- rep(0, length(probs))
pred.glm[probs > 0.5] <- 1
table(pred.glm, crim_modified.test)
cm_lr=confusionMatrix(data = pred.glm, reference = crim_modified.test)
cm_lr$byClass
mean(pred.glm == crim_modified.test)
```

Better results and model is simpler. Moving to LDA: 

```{r}
# LDA: Model with all parameters
fit.lda <- lda(crim_modified ~ . - crim_modified  - crim, data = Boston, subset = train)
pred.lda <- predict(fit.lda, Boston.test)
table(pred.lda$class, crim_modified.test)
mean(pred.lda$class == crim_modified.test)
cm=confusionMatrix(data = pred.lda$class, reference = crim_modified.test)
cm$byClass
```

Poor results when we use all params.

```{r}
# LDA: Model with parameters selected by lasso
fit.lda <- lda(crim_modified ~ zn + indus + chas + nox + rm + dis + rad + ptratio + black + lstat + medv, data = Boston, subset = train)
pred.lda <- predict(fit.lda, Boston.test)
table(pred.lda$class, crim_modified.test)
mean(pred.lda$class == crim_modified.test)
cm=confusionMatrix(data = pred.lda$class, reference = crim_modified.test)
cm$byClass
```

Poor results when we use Lasso params. Logistic Reg performed better.

```{r}
# LDA: Model with parameters selected by forward selection
fit.lda <- lda(crim_modified ~ rad + black + lstat, data = Boston, subset = train)
pred.lda <- predict(fit.lda, Boston.test)
table(pred.lda$class, crim_modified.test)
mean(pred.lda$class == crim_modified.test)
cm_lda=confusionMatrix(data = pred.lda$class, reference = crim_modified.test)
cm_lda$byClass
```

Simplest model performed best with LDA. For both LR and LDA, simplest model performed the best. Between LDA and LR, LR has performed slightly better.

Moving to KNN.

```{r}
# KNN: Model with all parameters
train.X <- cbind(zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, lstat, medv)[train, ]
test.X <- cbind(zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, lstat, medv)[test, ]
train.crim_modified <- crim_modified[train]
set.seed(1)
pred.knn <- knn(train.X, test.X, train.crim_modified, k = 1)
table(pred.knn, crim_modified.test)
mean(pred.knn == crim_modified.test)
pred.knn <- knn(train.X, test.X, train.crim_modified, k = 10)
table(pred.knn, crim_modified.test)
mean(pred.knn == crim_modified.test)
pred.knn <- knn(train.X, test.X, train.crim_modified, k = 100)
table(pred.knn, crim_modified.test)
mean(pred.knn == crim_modified.test)
```

When using all params we get the best results when k =10

```{r}

# KNN: Model with parameters selected by lasso
train.X <- cbind(zn, indus, chas, nox, rm, dis, rad, ptratio, black, lstat, medv)[train, ]
test.X <- cbind(zn, indus, chas, nox, rm, dis, rad, ptratio, black, lstat, medv)[test, ]
train.crim_modified <- crim_modified[train]
set.seed(1)
pred.knn <- knn(train.X, test.X, train.crim_modified, k = 1)
table(pred.knn, crim_modified.test)
mean(pred.knn == crim_modified.test)
pred.knn <- knn(train.X, test.X, train.crim_modified, k = 10)
table(pred.knn, crim_modified.test)
mean(pred.knn == crim_modified.test)
pred.knn <- knn(train.X, test.X, train.crim_modified, k = 100)
table(pred.knn, crim_modified.test)
mean(pred.knn == crim_modified.test)
```

When using lasso params results are similar. The k=10 mean actually falls. 

```{r}
# KNN: Model with parameters selected by forward selection
train.X <- cbind(rad, black, lstat)[train, ]
test.X <- cbind(rad, black, lstat)[test, ]
train.crim_modified <- crim_modified[train]
set.seed(1)
pred.knn <- knn(train.X, test.X, train.crim_modified, k = 1)
table(pred.knn, crim_modified.test)
mean(pred.knn == crim_modified.test)
pred.knn <- knn(train.X, test.X, train.crim_modified, k = 10)
table(pred.knn, crim_modified.test)
mean(pred.knn == crim_modified.test)
pred.knn <- knn(train.X, test.X, train.crim_modified, k = 100)
table(pred.knn, crim_modified.test)
mean(pred.knn == crim_modified.test)

cm_lr$byClass
cm_lda$byClass
```

All in all it seems that the simple Logistic Regression model with parameters selected by forward selection performs best closely followed by LDA model with parameters selected by forward selection. One reason this is possible is that the underlying data has a relatively simple linear relationship with a few of the predictors. It doesn't have non-linearity and so with more flexible methods like KNN (and to a lesser extent LDA)

### 2. Now repeat the exercise but classify those neighborhoods in the bottom 10% percentile with lowest crime rates. What differences do you notice between this and the previous classification task (hint: look at the confusion matrix)? Why may it be deceiving to only look at misclassification rate? What other measures can you consider?


```{r}
# reload data and do some data processing to prepare the categorical variable
data(Boston)
crim_modified <- rep(0, length(crim))
crim_modified[which(Boston$crim <= quantile(Boston$crim, 0.1))] <- 1

# add new column to the data frame
Boston <- data.frame(Boston, crim_modified)

n=nrow(Boston)
train = sample(seq(1:n),round(n * 0.75), replace = FALSE)
test = setdiff(1:n, train)

Boston.train = Boston[ train,]
Boston.test = Boston[test,]
crim_modified.test <- crim_modified[test]

#test = which(Boston$crim <= quantile(Boston$crim, 0.1))
#train = which(Boston$crim > quantile(Boston$crim, 0.1))

# Logistic Regression: Model with all parameters
fit.glm <- glm(crim_modified ~ . - crim_modified - crim, data = Boston, family = binomial, subset = train)
probs <- predict(fit.glm, Boston.test, type = "response")
pred.glm <- rep(0, length(probs))
pred.glm[probs > 0.5] <- 1
conf_matrix = table(pred.glm, crim_modified.test)
conf_matrix
mean(pred.glm == crim_modified.test)
cm=confusionMatrix(data = pred.glm, reference = crim_modified.test)
cm$byClass
```

We can see the problem right here that - There are no predictions for '1'. The (mis)classification rate is high but doesn't tell the complete story. We have to look at Sensitivity, Specificity, Precision and Recall to get a complete picture. Lets continue:

```{r}
# Logistic Regression: Model with parameters selected by lasso
fit.glm <- glm(crim_modified ~ zn + indus + chas + nox + rm + dis + rad + ptratio + black + lstat + medv, data = Boston, family = binomial, subset = train)
probs <- predict(fit.glm, Boston.test, type = "response")
pred.glm <- rep(0, length(probs))
pred.glm[probs > 0.5] <- 1
table(pred.glm, crim_modified.test)
mean(pred.glm == crim_modified.test)
cm=confusionMatrix(data = pred.glm, reference = crim_modified.test)
cm$byClass

```

Results similar to when we use all params= but slightly better. Note that Specificity is still almost 0.

```{r}
# Logistic Regression: Model with parameters selected by forward selection
fit.glm <- glm(crim_modified ~ rad + black + lstat, data = Boston, family = binomial, subset = train)
probs <- predict(fit.glm, Boston.test, type = "response")
pred.glm <- rep(0, length(probs))
pred.glm[probs > 0.5] <- 1
table(pred.glm, crim_modified.test)
mean(pred.glm == crim_modified.test)
cm=confusionMatrix(data = pred.glm, reference = crim_modified.test)
cm$byClass
```

Same issue persists. Moving to LDA:

```{r}
# LDA: Model with all parameters
fit.lda <- lda(crim_modified ~ . - crim_modified  - crim, data = Boston, subset = train)
pred.lda <- predict(fit.lda, Boston.test)
table(pred.lda$class, crim_modified.test)
mean(pred.lda$class == crim_modified.test)
cm=confusionMatrix(data = pred.lda$class, reference = crim_modified.test)
cm$byClass
```

Slightly better but specificity is still almost 0.

```{r}
# LDA: Model with parameters selected by lasso
fit.lda <- lda(crim_modified ~ zn + indus + chas + nox + rm + dis + rad + ptratio + black + lstat + medv, data = Boston, subset = train)
pred.lda <- predict(fit.lda, Boston.test)
table(pred.lda$class, crim_modified.test)
mean(pred.lda$class == crim_modified.test)
cm=confusionMatrix(data = pred.lda$class, reference = crim_modified.test)
cm$byClass
```

Virtually same as the LDA with all params.

```{r}
# LDA: Model with parameters selected by forward selection
fit.lda <- lda(crim_modified ~ rad + black + lstat, data = Boston, subset = train)
pred.lda <- predict(fit.lda, Boston.test)
table(pred.lda$class, crim_modified.test)
mean(pred.lda$class == crim_modified.test)
cm=confusionMatrix(data = pred.lda$class, reference = crim_modified.test)
cm$byClass
```

We continue to see the same issue.

Moving to KNN.

```{r}
# KNN: Model with all parameters
train.X <- cbind(zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, lstat, medv)[train, ]
test.X <- cbind(zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, lstat, medv)[test, ]
train.crim_modified <- crim_modified[train]
set.seed(1)
pred.knn <- knn(train.X, test.X, train.crim_modified, k = 1)
table(pred.knn, crim_modified.test)
mean(pred.knn == crim_modified.test)
cm=confusionMatrix(data = pred.knn, reference = crim_modified.test)
cm$byClass
```

The results have improved substantially as the specificity is now giving decent results. Continue further:

```{r}
pred.knn <- knn(train.X, test.X, train.crim_modified, k = 10)
table(pred.knn, crim_modified.test)
mean(pred.knn == crim_modified.test)
cm=confusionMatrix(data = pred.knn, reference = crim_modified.test)
cm$byClass

pred.knn <- knn(train.X, test.X, train.crim_modified, k = 100)
table(pred.knn, crim_modified.test)
mean(pred.knn == crim_modified.test)
cm=confusionMatrix(data = pred.knn, reference = crim_modified.test)
cm$byClass
```

We had deccent results with k=1 and k=10. However with k=100 we have tjhe old issue of having specificity almost or exactly 0

```{r}

# KNN: Model with parameters selected by lasso
train.X <- cbind(zn, indus, chas, nox, rm, dis, rad, ptratio, black, lstat, medv)[train, ]
test.X <- cbind(zn, indus, chas, nox, rm, dis, rad, ptratio, black, lstat, medv)[test, ]
train.crim_modified <- crim_modified[train]
set.seed(1)
pred.knn <- knn(train.X, test.X, train.crim_modified, k = 1)
table(pred.knn, crim_modified.test)
mean(pred.knn == crim_modified.test)
cm=confusionMatrix(data = pred.knn, reference = crim_modified.test)
cm$byClass
```
Again we see results getting better.

```{r}
pred.knn <- knn(train.X, test.X, train.crim_modified, k = 10)
table(pred.knn, crim_modified.test)
mean(pred.knn == crim_modified.test)
cm=confusionMatrix(data = pred.knn, reference = crim_modified.test)
cm$byClass
pred.knn <- knn(train.X, test.X, train.crim_modified, k = 100)
table(pred.knn, crim_modified.test)
mean(pred.knn == crim_modified.test)
cm=confusionMatrix(data = pred.knn, reference = crim_modified.test)
cm$byClass
```

We had deccent results with k=1 and k=10. However with k=100 we have tjhe old issue of having specificity almost or exactly 0

```{r}
# KNN: Model with parameters selected by forward selection
train.X <- cbind(rad, black, lstat)[train, ]
test.X <- cbind(rad, black, lstat)[test, ]
train.crim_modified <- crim_modified[train]
set.seed(1)
pred.knn <- knn(train.X, test.X, train.crim_modified, k = 1)
table(pred.knn, crim_modified.test)
mean(pred.knn == crim_modified.test)
cm=confusionMatrix(data = pred.knn, reference = crim_modified.test)
cm$byClass
pred.knn <- knn(train.X, test.X, train.crim_modified, k = 10)
table(pred.knn, crim_modified.test)
mean(pred.knn == crim_modified.test)
cm=confusionMatrix(data = pred.knn, reference = crim_modified.test)
cm$byClass
pred.knn <- knn(train.X, test.X, train.crim_modified, k = 100)
table(pred.knn, crim_modified.test)
mean(pred.knn == crim_modified.test)
cm=confusionMatrix(data = pred.knn, reference = crim_modified.test)
cm$byClass
```

In summary we got the best results with all parameters and using k=1